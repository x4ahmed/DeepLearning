{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa8c4b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comments on the Solution of Task 1 and Task 2\n",
    "\n",
    "## Task 1\n",
    "\n",
    "We noticed that the $x_1$ and $x_2$ values in the dataset range between $-1$ and $1$, since we know that the sigmoid activation function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ outputs exactly $0.5$ when the input is $0$, greater than $0.5$ for positive inputs, and less than $0.5$ for negative inputs. <br>\n",
    "\n",
    "The approach was to determine the minimum values for $\\alpha_0$, $\\alpha_1$, and $\\alpha_2$ that result in $\\alpha_0 + \\alpha_1 x_{i1} + \\alpha_2 x_{i2} \\geq 0$ for the points $(0,1)$ and $(-1,0)$ in order to be correctly segmented to class 1.<br>\n",
    "\n",
    "Since the class 1 points lie along the positive $y$-axis and negative $x$-axis, the reasonable prediction was to choose a negative $\\alpha_1$ and a positive $\\alpha_2$, so that the weight $\\alpha_1$ compensates for any negative $x_1$ input, resulting in a positive or zero output to be passed to the sigmoid activation function. <br>\n",
    "Since $(0,0)$ needs to be classified in the class 0, $\\alpha_0$ needs to be negative.\n",
    "\n",
    "Based on the previous observations we assumed $\\alpha_0 = -1$, $\\alpha_1 = -1$, and $\\alpha_2 = 1$. <br>\n",
    "Validation test case $(-1,0)$: $\\hat{y} = \\sigma(-1 + (-1)(-1) + (1)(0)) = \\sigma(-1 + 1) = \\sigma(0)$, since $\\sigma(0) = 0.5 \\geq 0.5$, this successfully predicts class 1. <br>\n",
    "After testing using the provided function: the previous weights and bias values resulted in correct classifications, but we adjusted them to achieve scores closer to $1$ for class 1 and closer to $0$ for class 0.\n",
    "\n",
    "## Task 2\n",
    "Similar approach was used in task 2, but this time we need to optimize for negative $y$-axis and positive $x$-axis, at the same time we need to exclude all the points that share the same $x_1 = 0$. <br>\n",
    "The bias $\\beta_0$ needs also to be negative to exclude the origin $(0,0)$ from classification $1$ <br>\n",
    "$\\beta_1$ needs to be positive since we are optimizing for positive $x-axis$ <br>\n",
    "$\\beta_2$ needs to be negative since we are optimizing for negative $y-axis$ <br>\n",
    "\n",
    "Assumption: $\\beta_0 = -1$, $\\beta_1 = 1$, and $\\beta_2 = -1$. <br>\n",
    "Validation test case $(0,-1)$: $\\hat{y} = \\sigma(-1 + (1)(0) + (-1)(-1)) = \\sigma(-1 + 1) = \\sigma(0)$, since $\\sigma(0) = 0.5 \\geq 0.5$, this successfully predicts class 1. <br>\n",
    "After testing using the provided function: the previous weights and bias values resulted in correct classifications, but we adjusted them to achieve scores closer to $1$ for class 1 and closer to $0$ for class 0.# Comments on the Solution of Task 1 and Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad002f231d5baa1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-10-21T18:33:14.128480Z",
     "start_time": "2025-10-21T18:33:13.680308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ],
   "id": "ce954b00",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Tensor\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "39d34348",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1 (5P)\n",
    "\n",
    "Suppose you have five input points, $\\textbf{x}_1=(0,0)^\\top$, $\\textbf{x}_2=(1,0)^\\top$,\n",
    "$\\textbf{x}_3=(0,-1)^\\top$, $\\textbf{x}_4=(-1,0)^\\top$ and $\\textbf{x}_5=(0,1)^\\top$, and\n",
    "the corresponding classes are $y_1=y_2=y_3=0$ and $y_4=y_5=1$:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b02f5388",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x = torch.tensor([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, -1],\n",
    "    [-1, 0],\n",
    "    [0, 1]\n",
    "])\n",
    "y = torch.tensor([0, 0, 0, 1, 1])\n",
    "labs = ['$x_1$', '$x_2$', '$x_3$', '$x_4$', '$x_5$']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cac260d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def plot_scatter(x: torch.tensor, y: torch.tensor, labs: torch.tensor) -> None:\n",
    "    \"\"\"Utility function to plot a scatter plot of the data\"\"\"\n",
    "    # copy the input tensors to avoid modifying the original ones\n",
    "    x = x.clone().detach().numpy()\n",
    "    y = y.clone().detach().numpy()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y)\n",
    "    for i, lab in enumerate(labs):\n",
    "        plt.annotate(\n",
    "            lab, (x[i, 0], x[i, 1]), size=16,\n",
    "            xytext=(2, 2), textcoords='offset points'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_scatter(x, y, labs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44d791a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Consider a logistic regression model\n",
    "$\\hat{y}_i=\\sigma\\left(\\alpha_0+\\alpha_1x_{i1}+\\alpha_2x_{i2}\\right)$, with\n",
    "$\\sigma(\\cdot)$ the sigmoid function, $\\sigma(x)=\\left(1+e^{-x}\\right)^{-1}$.\n",
    "What values for $\\alpha_0$, $\\alpha_1$ and $\\alpha_2$ would result in the correct\n",
    "classification for this dataset? A positive label is predicted when the output of the\n",
    "sigmoid is larger or equal than 0.5.\n",
    "\n",
    "**Note**: do not use any formulas or automated methods to find the answer.\n",
    "Think for yourself. A logistic regression classifier is nothing more than a hyper-plane\n",
    "separating points of the two classes. If necessary, review vectors, dot-products and\n",
    "their geometrical interpretation in linear algebra. This applies to the following\n",
    "exercises, too.\n",
    "\n",
    "We add a first column of ones, which is used for the 'bias'."
   ]
  },
  {
   "cell_type": "code",
   "id": "30bdac3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x_design = torch.cat([torch.ones(5, 1), x], dim=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5bff2c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "a0 = -8\n",
    "\n",
    "a1 = -16\n",
    "\n",
    "a2 = 16\n",
    "\n",
    "a = torch.tensor([a0, a1, a2], dtype=torch.float)\n",
    "\n",
    "# We define a custom sigmoid function\n",
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "# Calculate predictions\n",
    "scores = sigmoid(x_design @ a)\n",
    "\n",
    "# Let's investigate the obtained scores.\n",
    "def print_scores(target: Tensor, scores: Tensor) -> None:\n",
    "    print('\\tTarget\\tScore')\n",
    "    [print('{}\\t{}\\t{:.3f}'.format('x' + str(i), int(t), float(s)))\n",
    "     for i, (t, s) in enumerate(zip(target, scores), start=1)]\n",
    "\n",
    "print_scores(y, scores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52cb2192",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should make sure that the last two values are close to one and the others\n",
    "are close to zero.\n",
    "\n",
    "**Note:** There are many valid parametrization that lead to a separating hyperplane. How would you prioritize between them?\n",
    "\n",
    "\n",
    "## Exercise 2 (5P)\n",
    "\n",
    "Continuing from the previous exercise, suppose now that $y_2=y_3=1$ and $y_1=y_2=y_5=0$."
   ]
  },
  {
   "cell_type": "code",
   "id": "9146ca79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "y = torch.tensor([0, 1, 1, 0, 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec90a34f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plot_scatter(x, y, labs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b7317ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Consider the same logistic regression model above with coefficients $\\beta_0$, $\\beta_1$\n",
    "and $\\beta_2$, how would you need to set these coefficients to correctly classify this\n",
    "dataset?"
   ]
  },
  {
   "cell_type": "code",
   "id": "f27d7914",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "b0 = -6\n",
    "\n",
    "b1 = 13\n",
    "\n",
    "b2 = -13\n",
    "\n",
    "b = torch.tensor([b0, b1, b2], dtype=torch.float)\n",
    "\n",
    "print_scores(y, sigmoid(x_design @ b))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
